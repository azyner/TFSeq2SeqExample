# Linear Sequence to Sequence Example in TensorFlow #

I wrote this example because I could not find any good examples of non-categorical sequence to sequence (seq2seq) models written in TensorFlow. There is only a single example in the TensorFlow library, and it focuses on text analysis. As such, most of the code is dealing with either vocabulary (word2vec) or varying sentence lengths (bucketing) which can obfiscate the underlying tutorial with regards to sequencing.

Sequence to Sequence motivating examples here

## Sequence to Sequence Recurrent Neural Networks ##

The primary difference between a standard model and a seq2seq model is the recurrent layer. A recurrent layer, is a layer that takes input from both the data being fed into the model, and the model passes internal states to itself between timesteps.

There are many different types of Recurrent Neural Networks (RNN), such as one to many, many to one, and many to many. THIS link provides a great explanation. As most tools in the TensorFlow library are focused on language, most tools in tensorflow/python/ops/seq2seq.py are using the many-to-many structure. In the TF tools there is a discrete handover point between observing the input (encoder) data, and generating output (decoder) data. This point is marked in the model by the "GO" symbol.

This example is written to simplify the example by removing the complicated tools used to digest language. To further simplify the structure, a linear (non-categorical) network is used. 

## Model Structure ##

This model is separated into the following sections:
1. Model Generation
  1. Input handling
  2. Output Generation
2. Data Generation

### Model Generation ###
The model consists of four general sections:
1. Input data (obervations)
2. The encoder sequence
3. The decoder sequence
4. The generated data

#DIAGRAM HERE#

#### Input Data ####



### Data Generation ###
The aim of the model is for it to predict a future track sequence, after it has been fed a sequence. 




### Loss Function ###
As a whole predictive track is generated by the model, the whole sequence is compared to the true data to generate a loss. This is different from other sequence generation techniques, such as by ALEX GRAVES, where only the next timestep is compared. In this model, a pairwise comparison is used, and is reduced by a mean square error to produce a single value.

RMSE CODE GOES HERE


